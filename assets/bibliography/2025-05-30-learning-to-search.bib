$ Main paper: Gflownets for finetuning LLM
@misc{hu2024amortizingintractableinferencelarge,
      title={Amortizing intractable inference in large language models}, 
      author={Edward J. Hu and Moksh Jain and Eric Elmoznino and Younesse Kaddar and Guillaume Lajoie and Yoshua Bengio and Nikolay Malkin},
      year={2024},
      eprint={2310.04363},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.04363}, 
}

$ Foundation of Gflownets first explained
@misc{bengio2023gflownetfoundations,
      title={GFlowNet Foundations}, 
      author={Yoshua Bengio and Salem Lahlou and Tristan Deleu and Edward J. Hu and Mo Tiwari and Emmanuel Bengio},
      year={2023},
      eprint={2111.09266},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.09266}, 
}

% Gflownet optimizations
@InProceedings{pmlr-v202-shen23a,
  title = 	 {Towards Understanding and Improving {GF}low{N}et Training},
  author =       {Shen, Max W and Bengio, Emmanuel and Hajiramezanali, Ehsan and Loukas, Andreas and Cho, Kyunghyun and Biancalani, Tommaso},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {30956--30975},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/shen23a/shen23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/shen23a.html},
  abstract = 	 {Generative flow networks (GFlowNets) are a family of algorithms that learn a generative policy to sample discrete objects $x$ with non-negative reward $R(x)$. Learning objectives guarantee the GFlowNet samples $x$ from the target distribution $p^*(x) \propto R(x)$ when loss is globally minimized over all states or trajectories, but it is unclear how well they perform with practical limits on training resources. We introduce an efficient evaluation strategy to compare the learned sampling distribution to the target reward distribution. As flows can be underdetermined given training data, we clarify the importance of learned flows to generalization and matching $p^*(x)$ in practice. We investigate how to learn better flows, and propose (i) prioritized replay training of high-reward $x$, (ii) relative edge flow policy parametrization, and (iii) a novel guided trajectory balance objective, and show how it can solve a substructure credit assignment problem. We substantially improve sample efficiency on biochemical design tasks.}
}


$ Applied gradient-based policy learning with the Glflownet's trajectory balance objective.
@misc{niu2024gflownettrainingpolicygradients,
      title={GFlowNet Training by Policy Gradients}, 
      author={Puhua Niu and Shili Wu and Mingzhou Fan and Xiaoning Qian},
      year={2024},
      eprint={2408.05885},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.05885}, 
}


@InProceedings{pmlr-v202-hu23c,
  title = 	 {{GF}low{N}et-{EM} for Learning Compositional Latent Variable Models},
  author =       {Hu, Edward J and Malkin, Nikolay and Jain, Moksh and Everett, Katie E and Graikos, Alexandros and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {13528--13549},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/hu23c/hu23c.pdf},
  url = 	 {https://proceedings.mlr.press/v202/hu23c.html},
  abstract = 	 {Latent variable models (LVMs) with discrete compositional latents are an important but challenging setting due to a combinatorially large number of possible configurations of the latents. A key tradeoff in modeling the posteriors over latents is between expressivity and tractable optimization. For algorithms based on expectation-maximization (EM), the E-step is often intractable without restrictive approximations to the posterior. We propose the use of GFlowNets, algorithms for sampling from an unnormalized density by learning a stochastic policy for sequential construction of samples, for this intractable E-step. By training GFlowNets to sample from the posterior over latents, we take advantage of their strengths as amortized variational inference algorithms for complex distributions over discrete structures. Our approach, GFlowNet-EM, enables the training of expressive LVMs with discrete compositional latents, as shown by experiments on non-context-free grammar induction and on images using discrete variational autoencoders (VAEs) without conditional independence enforced in the encoder.}
}

% Approximates the initial posterior using a standard GFlowNet and subsequently updates it using a tailored procedure that requires only the newly observed data. Significantly faster than repeatedly training a GFlowNet from scratch to sample from the full posterior.
@inproceedings{NEURIPS2024_2fb57276,
 author = {da Silva, Tiago and de Souza, Daniel Augusto and Mesquita, Diego},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {27153--27177},
 publisher = {Curran Associates, Inc.},
 title = {Streaming Bayes GFlowNets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/2fb57276bfbaf1b832d7bfcba36bb41c-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}

@misc{malik2023batchgfngenerativeflownetworks,
      title={BatchGFN: Generative Flow Networks for Batch Active Learning}, 
      author={Shreshth A. Malik and Salem Lahlou and Andrew Jesson and Moksh Jain and Nikolay Malkin and Tristan Deleu and Yoshua Bengio and Yarin Gal},
      year={2023},
      eprint={2306.15058},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.15058}, 
}
